---
title: "Prediction Assignment"
author: "Justin Meyer"
date: "January 18, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## How I Built My Model

First I read in the csv files.

```{r}

testing <- read.csv("pml-testing.csv")
training <- read.csv("pml-training.csv")

```

Then I ran some descriptive statistics on the data. I chose these fields because they weren't missing any data and because they seemed the most relevant to the outcome that I want to predict.

```{r}

training <- subset(training, 
                   select = c("classe", 
            "roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt",
            "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm",
            "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "total_accel_dumbbell",
            "roll_forearm", "pitch_forearm", "yaw_forearm", "total_accel_forearm"))

testing <- subset(testing, 
                   select = c( 
            "roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt",
            "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm",
            "roll_dumbbell", "pitch_dumbbell", "yaw_dumbbell", "total_accel_dumbbell",
            "roll_forearm", "pitch_forearm", "yaw_forearm", "total_accel_forearm"))

summary(training)

```

Next I split the training set into a training set and a validation set since I can't use the provided test set to validate my model since the test set doesn't include the classe field.

```{r}

data <- training
rm(training)

library(caret)
set.seed(73)

trainIndex <- createDataPartition(data$classe, 
                                  p = .8, # Proportion in the index
                                  list = FALSE, # Output type
                                  times = 1) # Number of partitions

training <- data[trainIndex,]
validation  <- data[-trainIndex,]

rm(data)

```

Next I created a random forest model based on the training data. I chose a random forest because it provided accurate results on the training data and because it is easy to interpret. The model predicts the field "classe" using the other available fields.

*How I used cross validation:* I used _k-fold cross validation_ to evaluate the accuracy of the model using the training data.

```{r}

library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

train_control <- trainControl(method = "cv", 
                              number = 3)

fit <- train(classe ~ ., 
                data = training, 
                method = "ctree",
                trControl = train_control)

```

The below output shows that the model performs well on the training data, with an accuracy of `r round(max(fit$results$Accuracy) * 100, 1)`%.

```{r}

fit

```

The following table shows the importance of the variables to the model.

```{r}

varImp(fit)

```

The following chart shows the relationship between p value threshold and accuracy.

```{r}

plot(fit)

```

Next I used the validation data set to check the accuracy of my model on new data. I made a prediction for each of the records in the validation set and then compared that to the actual "classe" value. *I expect that the out of sample error will be about 12% since accuracy on the validation set is about 88%* 

```{r}

validation_prediction <- predict(fit, validation)

confusion <- confusionMatrix(validation_prediction, validation$classe)

```

Finally, I use the random forest model to predict the class for each of the testing set records.

```{r}

predict(fit, testing)

```
